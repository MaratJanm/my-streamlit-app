import streamlit as st
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
import seaborn as sns
import io

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="–ê–Ω–∞–ª–∏–∑ –∫–ª–∏–µ–Ω—Ç–æ–≤", layout="wide")
st.title('üéØ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤')
st.write("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ XLSX")

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª", type=["xlsx"], key="file_uploader")

# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
@st.cache_data
def load_data(file):
    return pd.read_excel(file)

if uploaded_file is not None:
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        Retail_df = load_data(uploaded_file)
        
        required_columns = ['CustomerID', 'TotalSpend', 'NumTransactions', 'Frequency', 'UniqueCategories']
        if not all(col in Retail_df.columns for col in required_columns):
            st.error("‚ùå –û—à–∏–±–∫–∞: –í —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏")
            st.stop()

        Retail_df = Retail_df.dropna()
        if Retail_df.empty:
            st.error("‚ùå –û—à–∏–±–∫–∞: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏")
            st.stop()

        numeric_cols = ['TotalSpend', 'NumTransactions', 'Frequency', 'UniqueCategories']
        for col in numeric_cols:
            Retail_df[col] = pd.to_numeric(Retail_df[col], errors='coerce')
        
        if Retail_df[numeric_cols].isnull().any().any():
            st.error("‚ùå –û—à–∏–±–∫–∞: –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            st.stop()

        features = numeric_cols
        X = Retail_df[features]
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        clustering_method = st.sidebar.selectbox(
            "–ú–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
            ["K-Means", "DBSCAN", "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è", "Gaussian Mixture"],
            index=0
        )

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
        if clustering_method == "K-Means":
            n_clusters = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 2, 10, 3)
        elif clustering_method == "DBSCAN":
            eps = st.sidebar.slider("EPS (—Ä–∞–¥–∏—É—Å)", 0.1, 1.0, 0.5, 0.05)
            min_samples = st.sidebar.slider("Min Samples", 1, 10, 5)
        elif clustering_method == "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è":
            n_clusters = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 2, 10, 3)
            linkage = st.sidebar.selectbox("–°–≤—è–∑—å", ["ward", "complete", "average", "single"])
        elif clustering_method == "Gaussian Mixture":
            n_clusters = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 2, 10, 3)
            covariance_type = st.sidebar.selectbox(
                "–¢–∏–ø –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏", 
                ["full", "tied", "diag", "spherical"]
            )

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        if clustering_method == "K-Means":
            model = KMeans(n_clusters=n_clusters, random_state=42)
            labels = model.fit_predict(X_scaled)
        elif clustering_method == "DBSCAN":
            model = DBSCAN(eps=eps, min_samples=min_samples)
            labels = model.fit_predict(X_scaled)
        elif clustering_method == "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è":
            model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)
            labels = model.fit_predict(X_scaled)
        elif clustering_method == "Gaussian Mixture":
            model = GaussianMixture(n_components=n_clusters, covariance_type=covariance_type, random_state=42)
            labels = model.fit_predict(X_scaled)

        Retail_df['cluster_label'] = labels

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —à—É–º–∞ –¥–ª—è DBSCAN
        if clustering_method == "DBSCAN":
            unique_labels = set(labels)
            if -1 in unique_labels:
                st.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω —à—É–º: {list(labels).count(-1)} –æ–±—ä–µ–∫—Ç–æ–≤")

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        st.header("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        with st.container():
            col1, col2 = st.columns(2)
            
            with col1:
                fig1, ax = plt.subplots(figsize=(10, 6))
                scatter = ax.scatter(
                    Retail_df['TotalSpend'], 
                    Retail_df['Frequency'], 
                    c=Retail_df['cluster_label'], 
                    cmap='viridis'
                )
                ax.set_title('Total Spend vs Frequency')
                ax.set_xlabel('–û–±—â–∞—è —Å—É–º–º–∞ –ø–æ–∫—É–ø–æ–∫')
                ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞ –ø–æ–∫—É–ø–æ–∫')
                plt.colorbar(scatter, ax=ax)
                st.pyplot(fig1)
                plt.close(fig1)

            with col2:
                pca = PCA(n_components=2)
                X_pca = pca.fit_transform(X_scaled)
                
                fig2, ax = plt.subplots(figsize=(10, 6))
                scatter = ax.scatter(
                    X_pca[:, 0], 
                    X_pca[:, 1], 
                    c=Retail_df['cluster_label'], 
                    cmap='viridis'
                )
                ax.set_title('PCA –ø—Ä–æ–µ–∫—Ü–∏—è')
                ax.set_xlabel('–ì–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 1')
                ax.set_ylabel('–ì–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 2')
                plt.colorbar(scatter, ax=ax)
                st.pyplot(fig2)
                plt.close(fig2)

        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        st.header("üìå –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        
        feature_descriptions = {
            'TotalSpend': '–û–±—â–∞—è —Å—É–º–º–∞ –ø–æ–∫—É–ø–æ–∫',
            'NumTransactions': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π',
            'Frequency': '–ß–∞—Å—Ç–æ—Ç–∞ –ø–æ–∫—É–ø–æ–∫',
            'UniqueCategories': '–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π'
        }

        global_medians = Retail_df[features].median(numeric_only=True)

        def generate_cluster_description(cluster_data):
            description = []
            for feature in features:
                cluster_median = cluster_data[feature].median()
                global_median = global_medians[feature]
                
                diff = cluster_median - global_median
                diff_percent = abs(diff) / global_median * 100
                
                if diff > 0:
                    trend = "‚ñ≤ –í—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ"
                    emoji = "üî∫"
                else:
                    trend = "‚ñº –ù–∏–∂–µ —Å—Ä–µ–¥–Ω–µ–≥–æ" 
                    emoji = "üîª"
                
                if abs(diff_percent) > 20:
                    intensity = "–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ"
                    emoji = "‚ùó" + emoji
                elif abs(diff_percent) > 10:
                    intensity = "—É–º–µ—Ä–µ–Ω–Ω–æ"
                else:
                    intensity = "–Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ"
                    emoji = "‚ûñ"
                
                description.append(
                    f"{emoji} **{feature_descriptions[feature]}**: "
                    f"{trend} ({intensity}, {diff_percent:.1f}%)"
                )
            return "\n\n".join(description)

        for cluster_id in sorted(Retail_df['cluster_label'].unique()):
            cluster_data = Retail_df[Retail_df['cluster_label'] == cluster_id]
            
            with st.expander(f"## –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({len(cluster_data)} –∫–ª–∏–µ–Ω—Ç–æ–≤)", expanded=True):
                col1, col2 = st.columns([1, 3])
                
                with col1:
                    st.metric(
                        label="–î–æ–ª—è –æ—Ç –≤—Å–µ—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤",
                        value=f"{len(cluster_data)/len(Retail_df):.1%}"
                    )
                    st.write("**–ú–µ–¥–∏–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**")
                    st.dataframe(
                        cluster_data[features].median().to_frame().T.style.format("{:.1f}"),
                        hide_index=True
                    )
                
                with col2:
                    st.write("**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**")
                    st.markdown(generate_cluster_description(cluster_data))
                
                st.markdown("---")

        # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        st.header("üì• –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            Retail_df.to_excel(writer, index=False)
        output.seek(0)
        
        st.download_button(
            label="–°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏",
            data=output,
            file_name='clustered_customers.xlsx',
            mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        st.error(f"‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        st.stop()
else:
    st.info("‚ÑπÔ∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞")